{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_CIFAR10_batch(file):\n",
    "    import _pickle as cPickle\n",
    "    fileopen = open(file , \"rb\")\n",
    "    dict = cPickle.load(fileopen,encoding=\"latin1\")\n",
    "    fileopen.close\n",
    "    \n",
    "    return dict['data'].reshape(-1, 32, 32, 3), dict['labels']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading................\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading................\")\n",
    "batch_fns = [os.path.join(r\"C:\\Users\\ravul\\Downloads\\cifar-10-batches-py\", \"data_batch_\" + str(i)) for i in range(1,6)]\n",
    "data_Batches = [Load_CIFAR10_batch(fn) for fn in batch_fns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = np.vstack([data_Batches[i][0] for i in range(len(data_Batches))]).astype('float')\n",
    "labels_all = np.vstack([data_Batches[i][1] for i in range(len(data_Batches))]).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the data of 50000 samples in 92:8 ratio, 4000 samples are used for the train and test sets for this clasification\n",
    "StratifiedShuffleSplit is used to split dataset, It splits  the data by taking equal number of samples each class in a random manner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# spliting the whole data set into 92:8 \n",
    "seed = 7\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "datasplit = StratifiedShuffleSplit(1, test_size=0.08, random_state = seed) \n",
    "\n",
    "print(datasplit.get_n_splits(data_all, labels_all))\n",
    "\n",
    "for train_index, test_index in datasplit.split(data_all, labels_all):\n",
    "    split_data_92 , split_data_8 = data_all[train_index], data_all[test_index]\n",
    "    split_label_92, split_label_8 = labels_all[train_index], labels_all[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4000 samples in trun divided into 7:3 ratio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(2800, 32, 32, 3) (1200, 32, 32, 3) (2800,) (1200,)\n"
     ]
    }
   ],
   "source": [
    "train_test_split = StratifiedShuffleSplit(1, test_size=0.3, random_state=seed)\n",
    "\n",
    "print(train_test_split.get_n_splits(split_data_8, split_label_8))\n",
    "\n",
    "for train_index, test_index in train_test_split.split(split_data_8, split_label_8):\n",
    "    train_data_70, test_data_30 = split_data_8[train_index], split_data_8[test_index]\n",
    "    train_label_70, test_label_30 = split_label_8[train_index], split_label_8[test_index]\n",
    "    \n",
    "train_data = train_data_70\n",
    "test_data  = test_data_30\n",
    "train_label = train_label_70\n",
    "test_label  = test_label_30\n",
    "\n",
    "print(train_data.shape, test_data.shape, train_label.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2800, 32, 32, 3) (1200, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Normalizing images\n",
    "\n",
    "def Normalize(data, eps=1e-8):\n",
    "    \n",
    "    data -= data.mean(axis=(1,2,3), keepdims=True)\n",
    "    std = np.sqrt(data.var(axis=(1,2,3), ddof=1, keepdims=True))\n",
    "    std[std<eps] = 1\n",
    "    \n",
    "    data /= std\n",
    "    \n",
    "    return(data)\n",
    "\n",
    "\n",
    "train_data = Normalize(train_data)\n",
    "test_data = Normalize(test_data)\n",
    "\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 2800) (3072, 1200)\n",
      "(2800, 3072) (1200, 3072)\n"
     ]
    }
   ],
   "source": [
    "# Whitening ZCA zero-phase component analysis.\n",
    "\n",
    "train_data_flat = train_data.reshape(train_data.shape[0], -1).T\n",
    "test_data_flat = test_data.reshape(test_data.shape[0], -1).T\n",
    "print(train_data_flat.shape, test_data_flat.shape)\n",
    "train_data_flat_T =  train_data_flat.T\n",
    "test_data_flat_T = test_data_flat.T\n",
    "print(train_data_flat_T.shape, test_data_flat_T.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA - principal component analysis dimensionality reduction\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "train_data_pca = PCA(n_components=train_data_flat.shape[1]).fit_transform(train_data_flat)\n",
    "test_data_pca = PCA(n_components=test_data_flat.shape[1]).fit_transform(test_data_flat)\n",
    "\n",
    "train_data_pca = train_data_pca.T\n",
    "test_data_pca =  test_data_pca.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      " (2800, 32, 32, 3) \n",
      "Testdata\n",
      " (1200, 32, 32, 3)\n",
      "(2800, 30) (1200, 30)\n"
     ]
    }
   ],
   "source": [
    "# SVD sinfular value decompostion (Dimensionality reduction )\n",
    "\n",
    "print(\"Train data\\n\",train_data.shape,\"\\nTestdata\\n\", test_data.shape)\n",
    "\n",
    "from skimage import color \n",
    "\n",
    "def svdFeatures(input_data):\n",
    "    svd_array_inputdata = []\n",
    "    size = input_data.shape[0]\n",
    "    \n",
    "    for i in range(0,size):\n",
    "        img = color.rgb2gray(input_data[i])\n",
    "        \n",
    "        U, s, V = np.linalg.svd(img, full_matrices = False)\n",
    "        \n",
    "        S = [s[i] for i in range(30)]\n",
    "        svd_array_inputdata.append(S)\n",
    "        svd_Matrix_inputdata = np.matrix(svd_array_inputdata)\n",
    "        \n",
    "    return(svd_Matrix_inputdata)\n",
    "\n",
    "#Apply SVD for train and test data\n",
    "\n",
    "train_data_svd = svdFeatures(train_data)\n",
    "test_data_svd = svdFeatures(test_data)\n",
    "\n",
    "print(train_data_svd.shape, test_data_svd.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier\n",
    "\n",
    "Steps for the classification\n",
    "\n",
    "- Initialize the classifier\n",
    "- Train the classifier (All classifiers in the scikit-learn uses a fit(X,y) method to fit the model training for the given train data X and train label y)\n",
    "- Predict the target (Given an unlabeled observation X , the predict(X) returns the predicted label y)\n",
    "- Evaluate the classifier (The score(X,y) returns the score for the given test data X and test label y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM support vector machine\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "classifier = svm.SVC(gamma=.001, probability=True)\n",
    "\n",
    "classifier.fit(train_data_flat_T, train_label) # After being fitted, the model can then be used to predict the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction \n",
      " [3 7 6 ... 4 8 8] (1200,)\n",
      "Score =  0.38666666666666666\n"
     ]
    }
   ],
   "source": [
    "# predict and score\n",
    "\n",
    "predicted  =  classifier.predict(test_data_flat_T)\n",
    "print(\"Prediction \\n\", predicted, predicted.shape)\n",
    "\n",
    "score = classifier.score(test_data_flat_T, test_label)\n",
    "print(\"Score = \",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
